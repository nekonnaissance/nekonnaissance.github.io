<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Geometry of the Unbiased Variance Estimator | Paweł Lewulis</title>

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="alternate"
        type="application/atom+xml"
        title="Paweł Lewulis"
        href="/feed.xml">

  <!-- MathJax -->
  <script>
    window.MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,600;0,700;1,400;1,600&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,400;0,600;0,700;1,400;1,600&display=swap" rel="stylesheet">

</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Paweł Lewulis</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Geometry of the Unbiased Variance Estimator</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2026-01-20T00:00:00+01:00" itemprop="datePublished">
        Jan 20, 2026
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>The unbiased variance estimator is given by the well-known formula:</p>

\[\widehat{\sigma}^2 := \frac{1}{n-1} \sum_{i=1}^n \left( X_i - \overline{X} \right)^2 ,\]

<p>where $X_1,\dots,X_n$ are independent draws from the same distribution and $\overline{X}$ denotes the sample mean.</p>

<p>Every so often, someone in the room eventually asks:</p>

<div class="aside">
  <p><em>Why do we have $n-1$ in the denominator? The expression looks almost like an average of $n$ squared deviations, so why isn’t the denominator simply $n$?</em></p>
</div>

<p>There are two answers one usually hears:</p>

<ol>
  <li>Write down the bias $\mathbf{E}[\widehat{\sigma}^2] - \sigma^2$, expand the square, expand $\overline{X}$, expand all the sums, calculate. See? It’s $0$.</li>
  <li>Because there are only $n-1$ degrees of freedom among the $X_i$, since translating all measurements by the same amount leaves the variance unchanged.</li>
</ol>

<p>The first answer is formally correct, but it explains little. The second one may feel hand-wavy. Why should degrees of freedom matter at all? Where, exactly, did the missing dimension go?</p>

<p>Since the difference between $\frac{1}{n}$ and $\frac{1}{n-1}$ is most visible for small values of $n$, let us begin by examining what happens in that regime.</p>

<h3 id="some-intuition-case-n--1">Some Intuition: Case $n = 1$</h3>

<p>Suppose we have only a single random measurement, and we observe $X_1 = 10$.<br />
There are many pairs $(\mu,\sigma)$ consistent with such an observation, for example:</p>

<ol>
  <li>$\mu \approx 10$ and $\sigma$ very small,</li>
  <li>$\mu \approx 10$ and $\sigma \approx 50$.</li>
</ol>

<p>There is no way to decide which scenario is closer to the truth. The scale of $\sigma$ can vary by orders of magnitude while remaining compatible with the data. A single observation provides no reliable information about the variance, so an unbiased estimator of $\sigma^2$ cannot exist in this setting.</p>

<p>Now suppose $\mu = 15$ is known and we observe $X_1 = 10$. While we still cannot estimate $\sigma$ precisely, we at least gain information about its order of magnitude. For instance, it is unlikely that $\sigma \ll 1$. This is weak information, but it is no longer nothing.</p>

<p>When the exact value of the mean is known, we are granted a point of reference. For fixed $\mu$, we can construct a slightly different unbiased estimator:</p>

\[\widehat{\sigma}^2_{\mu\text{ known}} := \frac{1}{n} \sum_{i=1}^n \left( X_i - \mu \right)^2\]

<p>As we can see, the $-1$ disappears from the denominator. To summarize:</p>

<ul>
  <li>For $n = 1$, it makes sense that the variance cannot be estimated unless $\mu$ is known.</li>
  <li>Replacing $n$ with $n - 1$ in the denominator can be viewed as a premium paid for not knowing the exact value of $\mu$.</li>
</ul>

<p>The second point may seem vague for now. In the next section, we make this intuition more precise.</p>

<h3 id="orthogonal-projections">Orthogonal Projections</h3>

<p>When thinking about $\widehat{\sigma}^2$ geometrically, we may view it as the squared Euclidean norm of an $n$-dimensional vector, normalized by $n-1$. If we define the sample vector</p>

\[\boldsymbol{X} = [X_1, \dots, X_n]^\top\]

<p>and the center-of-mass vector</p>

\[\boldsymbol{C} = \overline{X}\,\mathbf{1}
= [\overline{X}, \dots, \overline{X}]^\top,\]

<p>where $\mathbf{1}$ stands for the all-ones vector, then</p>

\[\widehat{\sigma}^2
= \frac{1}{n-1}\,\lVert \boldsymbol{X} - \boldsymbol{C} \rVert_{\ell^2}^2.\]

<p>Here $\lVert \cdot \rVert_{\ell^2}$ denotes the Euclidean norm, the standard way mathematicians refer to “distance from zero”.</p>

<p>Neat. We are interested in the expected distance from $\boldsymbol{X} - \boldsymbol{C}$ to $\mathbf{0}$. Let us notice a simple but important property of this vector.<br />
If we define $X_i’ = X_i - \overline{X}$ for $i = 1, \dots, n$, then</p>

\[\langle \boldsymbol{X} - \boldsymbol{C}, \mathbf{1} \rangle
= X_1' + \dots + X_n'
= \left( X_1 - \frac{X_1 + \dots + X_n}{n} \right)
+ \dots
+ \left( X_n - \frac{X_1 + \dots + X_n}{n} \right)
= 0.\]

<p>This means that the random vectors $\boldsymbol{X} - \boldsymbol{C}$ lie in a subspace orthogonal to the all-ones vector $\mathbf{1}$. Since $\boldsymbol{C}$ itself is a multiple of $\mathbf{1}$, subtracting it removes the component of $\boldsymbol{X}$ in the $\mathbf{1}$ direction. Consequently, $\boldsymbol{X} - \boldsymbol{C}$ is the orthogonal projection of $\boldsymbol{X}$ onto the hyperplane</p>

\[X_1 + \dots + X_n = 0.\]

<p>The discussion above shows that subtracting the mean in the definition of $\widehat{\sigma}^2$ removes exactly the component of $\boldsymbol{X}$ in the $\mathbf{1}$ direction. Let us take a look at a simple case in dimension two.</p>

<p><img src="/assets/img/variance/projection_single.png" alt="Variance estimator animation" style="max-width: 600px; width: 100%; display: block; margin: 1.5em auto;" /></p>

<p>Even though the scenario depicted above may appear overly simplistic, it still allows for a few useful observations.</p>

<ul>
  <li>If $\mu = 0$, the squared distance of the blue point from the origin represents $n\,\widehat{\sigma}^2_{\mu\text{ known}}$. Changing $\mu$ simply translates the point along the $[1,1]^\top$ direction, while its projection remains fixed.</li>
  <li>Similarly, the squared distance of the red point from the origin represents $(n-1)\,\widehat{\sigma}^2$, where we pretend that the true value of $\mu$ remains unknown.</li>
</ul>

<p>Basic trigonometry shows that in the $\mu = 0$ case we have</p>

\[\lVert \boldsymbol{X} \rVert_{\ell^2}
= \lVert \boldsymbol{X} - \boldsymbol{C} \rVert_{\ell^2} \cos(\alpha),\]

<p>which implies</p>

\[n\,\widehat{\sigma}^2_{\mu\text{ known}}
= (n-1)\,\widehat{\sigma}^2 \cos(\alpha).\]

<p>Now observe that replacing $n-1$ with $n$ in the definition of $\widehat{\sigma}^2$ would lead to the highly dubious identity</p>

\[\widehat{\sigma}^2_{\mu\text{ known}}
\stackrel{?!}{=}
\widehat{\sigma}^2 \cos(\alpha).\]

<p>Since $\lvert \cos(\alpha) \rvert \leq 1$, this would systematically push the variance estimate downward.
This is precisely the bias we want to avoid.</p>

<p>At this stage, one might suspect that $\frac{1}{n-1}$ is the correct normalization factor when constructing $\widehat{\sigma}^2$. While this is not yet a proof, it already makes clear that using $\frac{1}{n}$ is the wrong choice.</p>

<h2 id="more-intuition-case-n--2">More Intuition: Case $n = 2$</h2>

<p>In this experiment we sample various values of $\mu$ from the interval $[-3, 3]$.
For each of these means we draw $(X_1, X_2)$ pairs $500$ times, where
$X_1$ and $X_2$ come from the normal distribution $\mathcal{N}(\mu, 1)$.
For each pair, we estimate the variance by computing either
$\widehat{\sigma}^2$ or $\widehat{\sigma}^2_{\mu \text{ known}}$.</p>

<p>We proceed in the same way as in the previous subsection:</p>

<ul>
  <li>To obtain $\widehat{\sigma}^2_{\mu \text{ known}}$, we compute the distance
from $(X_1, X_2)$ to $(\mu, \mu)$.</li>
  <li>To obtain $\widehat{\sigma}^2$, we compute the distance from the origin to the
projection of $(X_1, X_2)$ onto the line $X_1 + X_2 = 0$.</li>
</ul>

<p><img src="/assets/mu_shift.gif" alt="Variance estimator animation" style="max-width: 600px; width: 100%; display: block; margin: 1.5em auto;" /></p>

<p>As the animation shows, both estimators remain constant as $\mu$ varies.
This is expected: $\widehat{\sigma}^2_{\mu \text{ known}}$ is proportional to
the distance from the center of the blue mass, so translating it changes nothing,
while $\widehat{\sigma}^2$ is based on projections, and translating the blue mass
along the $[1,1]^\top$ direction does not affect its projection.</p>

<p>We observe that the deviation of the red dots is systematically smaller, since one
direction is removed entirely. This is precisely the geometric origin of the
$n - 1$ degrees of freedom.</p>

<p>What is the main source of this discrepancy? Consider a blue point
$\boldsymbol{X} = (4.01, 4.03)$ under $\mu = 2$. Being far from the center of the
blue mass, it contributes substantially to the average deviation. After
projection, however, the situation looks different. The same point is mapped to
$\boldsymbol{P}(\boldsymbol{X}) = (-0.01, 0.01)$, which lies very close to
$\mathbf{0}$.</p>

<p>This reflects the fact that observing $X_1 = 4.01$ and $X_2 = 4.03$ with $\mu$
unknown naturally suggests values concentrated around $4$ and a small variance.
While $n = 2$ hardly constitutes a meaningful sample, this interpretation is at
least reasonable if one is forced to make a judgment.</p>

<p>To observe a similar effect for larger samples (namely, all measurements clustering
together while remaining far from the true mean) we would need to be rather unlucky.
This explains why replacing $\frac{1}{n}$ with $\frac{1}{n-1}$ becomes almost
irrelevant for large $n$, but crucial when the sample is small.</p>

<h3 id="geometric-argument-made-formal">Geometric Argument, Made Formal</h3>

<p>Let’s play around a bit with the expression</p>

\[\widehat{\theta} := \sum_{i=1}^n 
\left( X_i - \overline{X} \right)^2 ,\]

<p>As we have already seen, the sum looks like an $\ell^2$ norm of something. To make this precise, let’s re-introduce the following notation:</p>

\[\text{the sample vector}\quad
\boldsymbol{X} =
\begin{bmatrix}
X_{1} \\
\vdots \\
X_{n}
\end{bmatrix},
\qquad
\text{the all-ones vector}\quad
\boldsymbol{1} =
\begin{bmatrix}
1 \\
\vdots \\
1
\end{bmatrix}.\]

<p>We can then form the vector $\boldsymbol{X} - \boldsymbol{1}\overline{X}$, noting that $\overline{X}$ is just a scalar:</p>

\[\boldsymbol{X} - \boldsymbol{1} \overline{X} 
=
\begin{bmatrix}
X_{1} - \overline{X}  \\
\vdots \\
X_{n} - \overline{X} 
\end{bmatrix}.\]

<p>We can now see that $\widehat{\theta}$ is obtained by summing the squares of the entries of the vector above. That gives us</p>

\[\widehat{\theta}
=
\| \boldsymbol{X} - \boldsymbol{1} \overline{X} \|_{\ell^2}^2.\]

<p>This observation also allows us to rewrite $\widehat{\theta}$ in a more compact way, more suitable for matrix calculations:</p>

\[\widehat{\theta}
=
\langle \boldsymbol{X} - \boldsymbol{1} \overline{X},
\boldsymbol{X} - \boldsymbol{1} \overline{X} \rangle
=
\left( \boldsymbol{X} - \boldsymbol{1} \overline{X} \right)^\top
\left( \boldsymbol{X} - \boldsymbol{1} \overline{X} \right).\]

<p>We notice that the expression $\boldsymbol{X} - \boldsymbol{1} \overline{X}$ is meaningful in some way. It would be neat to have a linear operator $\boldsymbol{P}$ that satisfies</p>

\[\boldsymbol{P} \boldsymbol{X}
=
\boldsymbol{X} - \boldsymbol{1} \overline{X},\]

<p>so we could study its properties and hopefully get some insights. To get there, we need to describe $\overline{X}$ in terms of $\boldsymbol{X}$:</p>

\[\overline{X}
=
\frac{1}{n} \left( X_1 + \dots + X_n \right)
=
\frac{1}{n} \boldsymbol{1}^\top \boldsymbol{X}.\]

<p>Of course, the dot product above could be also described as $\boldsymbol{X}^\top \boldsymbol{1}$, but we need $\boldsymbol{X}$ itself not to be transposed. Then,</p>

\[\boldsymbol{X} - \overline{X} \boldsymbol{1}
=
\boldsymbol{X}
-
\frac{1}{n} \boldsymbol{1} \boldsymbol{1}^\top \boldsymbol{X}
=
\underbrace{
\left( \mathbf{id} - \frac{1}{n} \boldsymbol{1}\boldsymbol{1}^\top \right)
}_{\boldsymbol{P}}
\boldsymbol{X}.\]

<p>Just for the record, the matrix $\boldsymbol{P}$ is:</p>

\[\boldsymbol{P}
=
\begin{bmatrix}
1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 1 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 1
\end{bmatrix}
-
\frac{1}{n}
\begin{bmatrix}
1 &amp; 1 &amp; \cdots &amp; 1 \\
1 &amp; 1 &amp; \cdots &amp; 1 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; 1 &amp; \cdots &amp; 1
\end{bmatrix}
=
\begin{bmatrix}
1-\frac{1}{n} &amp; -\frac{1}{n} &amp; \cdots &amp; -\frac{1}{n} \\
-\frac{1}{n} &amp; 1-\frac{1}{n} &amp; \cdots &amp; -\frac{1}{n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
-\frac{1}{n} &amp; -\frac{1}{n} &amp; \cdots &amp; 1-\frac{1}{n}
\end{bmatrix}.\]

<p>Fine, we succeeded in creating an operator $\boldsymbol{P}$ of the desired property. We get</p>

\[\widehat{\theta}
=
(\boldsymbol{P} \boldsymbol{X})^\top (\boldsymbol{P} \boldsymbol{X})
=
\boldsymbol{X}^\top \boldsymbol{P}^\top \boldsymbol{P} \boldsymbol{X}.\]

<p>The matrix/operator $\boldsymbol{P}$ has two critical properties:</p>

<ol>
  <li>It is symmetric, so $\boldsymbol{P}^\top = \boldsymbol{P}$.</li>
  <li>It effectively subtracts the mean (or the center of mass) from a vector. Doing it twice changes nothing, because the mean has already been removed after the first pass. Hence, $\boldsymbol{P}^2 = \boldsymbol{P}$.</li>
</ol>

<p>In other words, it’s an orthogonal projection (as already observed in the previous sections). Thus, we get</p>

\[\widehat{\theta}
=
\boldsymbol{X}^\top \boldsymbol{P} \boldsymbol{X}.\]

<p>To answer what factor fits an unbiased estimator of variance, we need to calculate the expected value of the expressions above. To this end, we need a supplementary result.</p>

<div class="lemma-box">

  <p><strong>Little Lemma</strong> $~$ Let $\boldsymbol{X}$ be a random vector of independent entries of length $n$ such that
$X_i \sim X$ for all the indices $i$, where $X$ is a random variable with well defined
expected value $\mu$ and variance $\sigma^2$. Let $\boldsymbol{A}$ be an $n \times n$
matrix. Then,</p>

\[\mathbf{E}\!\left[ \boldsymbol{X}^\top \boldsymbol{A} \boldsymbol{X} \right]
=
\mu^2 \Sigma_{\boldsymbol{A}} + \sigma^2 \operatorname{tr}(\boldsymbol{A}),\]

  <p>where $\Sigma_{\boldsymbol{A}}$ stands for the sum of all the entries of $\boldsymbol{A}$
and $\operatorname{tr}(\boldsymbol{A})$ for the trace of $\boldsymbol{A}$.</p>

</div>

<p><em>Proof.</em> $~$ Unfortunately, we need to simply expand. It doesn’t seem like there is anything more
clever we could pull off.</p>

\[\mathbf{E}\!\left[ \boldsymbol{X}^\top \boldsymbol{A} \boldsymbol{X} \right]
=
\sum_{i=1}^n \sum_{j=1}^n A_{ij} \mathbf{E} \left[ X_i X_j \right].\]

<p>Provided that $i \neq j$, we have</p>

\[\mathbf{E} \left[ X_i X_j \right]
=
\mathbf{E} \left[ X_i \right] \mathbf{E} \left[ X_j \right]
=
\mathbf{E} \left[ X \right]^2
=
\mu^2.\]

<p>On the other hand, if $i=j$, then
$\mathbf{E} \left[ X_i X_j \right] = \mathbf{E} \left[ X^2 \right]$ and we notice</p>

\[\sigma^2
=
\mathbf{E} \left[ (X - \mu)^2 \right]
=
\mathbf{E} \left[ X^2 \right] - \mu^2,\]

<p>which gives $\mathbf{E} \left[ X^2 \right] = \sigma^2 + \mu^2$.</p>

<p>This gives us</p>

\[\mathbf{E}\!\left[ \boldsymbol{X}^\top \boldsymbol{A} \boldsymbol{X} \right]
=
\mu^2 \sum_{i=1}^n \sum_{j=1}^n A_{ij}
+
\sigma^2 \sum_{i=1}^n A_{ii}
=
\mu^2 \Sigma_{\boldsymbol{A}} + \sigma^2 \operatorname{tr}(\boldsymbol{A}).\]

<p><span class="qed">□</span></p>

<p>In our discussion we are interested in calculating
$\mathbf{E} \left[ \boldsymbol{X}^\top \boldsymbol{P} \boldsymbol{X} \right]$.
This turns out to be quite simple, because the sum of all the entries vanishes. We thus have</p>

\[\mathbf{E} [ \widehat{\theta} ]
=
\sigma^2 \operatorname{tr}(\boldsymbol{P})
=
(n-1)\sigma^2.\]

<p>That’s all. To make the estimator unbiased, we just divide $\widehat{\theta}$ by $n-1$.</p>

<h3 id="appendix-what-if-mu-is-known">Appendix: What if $\mu$ is known?</h3>

<p>Let us follow the same geometric argument. First, we note that</p>

\[\mathbf{E}[\widehat{\sigma}^2_{\text{$\mu$ known}}]
=
\mathbf{E}[\boldsymbol{X}^2] - \mu^2.\]

<p>We also observe that for the sample vector $\boldsymbol{X}$ it holds that</p>

\[\mathbf{E}[\boldsymbol{X}^2]
=
\frac{1}{n} \| \boldsymbol{X} \|_{\ell^2}
=
\frac{1}{n} \boldsymbol{X}^\top \boldsymbol{X}.\]

<p>In other words, we now deal with $\boldsymbol{X}^\top \boldsymbol{X}$ instead of
$\boldsymbol{X}^\top \boldsymbol{P} \boldsymbol{X}$. The projection matrix
$\boldsymbol{P}$ got replaced by the identity matrix $\mathbf{id}$.
Applying Little Lemma, we obtain</p>

\[\frac{1}{n}
\mathbf{E}\!\left[
\boldsymbol{X}^\top \boldsymbol{X}
\right]
=
\mu^2 + \sigma^2,\]

<p>since $\text{tr}(\mathbf{id}) = n$. This yields</p>

\[\mathbf{E}[\widehat{\sigma}^2_{\text{$\mu$ known}}]
=
\mu^2 + \sigma^2 - \mu^2
=
\sigma^2.\]

<p>The means cancel out, as they should, since the variance has nothing to do with
$\mu$. The estimator $\widehat{\sigma}^2_{\mu\ \text{known}}$ is unbiased, and the
factor $n-1$ simply does not show up here.</p>

  </div>

  <!-- Share bar -->
  <div class="share-bar">
    <a href="https://www.linkedin.com/sharing/share-offsite/?url=http://localhost:4000/2026/01/20/variance-estimator.html"
       target="_blank" rel="noopener">LinkedIn</a>
    <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/2026/01/20/variance-estimator.html"
       target="_blank" rel="noopener">Facebook</a>
  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Paweł Lewulis</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Paweł Lewulis</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Math, quant stuff, and other distractions.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
